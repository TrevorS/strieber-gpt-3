# ============================================================================
# strieber-gpt-3 Environment Configuration
# ============================================================================
#
# Copy this file to .env and customize for your environment
# The .env file is gitignored and should contain your actual configuration
#

# ============================================================================
# Model Configuration
# ============================================================================

# Model file name (must exist in ./models/ directory)
# Testing: gpt-oss-20b-Q4_K_M.gguf (~16GB, faster)
# Production: gpt-oss-120b-mxfp4-00001-of-00003.gguf (~63GB, better quality)
MODEL_FILE=gpt-oss-20b-Q4_K_M.gguf

# Context window size (in tokens)
# gpt-oss-20b: 4096-8192 recommended
# gpt-oss-120b: up to 131,072 (128k)
CONTEXT_SIZE=8192

# Number of layers to offload to GPU
# 999 = auto-detect maximum (works for both 20b and 120b)
# 70 = all layers for gpt-oss-120b specifically
GPU_LAYERS=999

# ============================================================================
# Server Configuration
# ============================================================================

# Port to expose llama-server on host
# Default: 8000
LLAMA_PORT=8000

# Host interface for llama-server
# 0.0.0.0 = listen on all interfaces
# 127.0.0.1 = localhost only (more secure)
LLAMA_HOST=0.0.0.0

# ============================================================================
# Open WebUI Configuration
# ============================================================================

# Port to expose Open WebUI on host
# Default: 3000
OPENWEBUI_PORT=3000

# OpenAI API Base URL
# Points to llama-server OpenAI endpoint via Docker network
# Do NOT change this unless running llama-server on a different host
OPENAI_API_BASE_URL=http://llama-server:8000/v1

# OpenAI API Key (dummy for local usage)
# Required by Open WebUI, but any value works for local/internal testing
# Default: sk-open-webui-local
OPENAI_API_KEY=sk-open-webui-local

# ============================================================================
# MCP Tool Server Configuration
# ============================================================================

# Jina Reader local processing
# Set to 'false' to use only Jina API (no local model)
USE_LOCAL_READER=true

# Jina Reader API key (optional, for higher rate limits)
# Without key: 20 requests/minute
# With key: 500 requests/minute
# Get key at: https://jina.ai/
# JINA_API_KEY=your-api-key-here

# Brave Search API key (required for web search functionality)
# Get key at: https://brave.com/search/api/
# BRAVE_API_KEY=your-api-key-here

# ============================================================================
# Advanced Configuration (Optional)
# ============================================================================
# Uncomment and modify as needed

# Number of parallel requests to handle
# PARALLEL_REQUESTS=4

# Batch size for prompt processing
# BATCH_SIZE=512

# KV cache precision (f16, f32, q8_0, q4_0)
# Lower precision = less memory, slightly lower quality
# CACHE_TYPE_K=f16
# CACHE_TYPE_V=f16
