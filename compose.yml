# ============================================================================
# strieber-gpt-3: LLama-Server + Open WebUI Docker Compose
# ============================================================================
# Docker compose file for running llama.cpp inference engine with Open WebUI
# on a GPU machine (e.g., DGX Spark)
#
# Usage:
#   docker compose up -d          # Start both services
#   docker compose up -d llama-server   # Start only llama-server
#
# Access:
#   - llama-server OpenAI API: http://localhost:8000
#   - Open WebUI: http://localhost:3000
#
# Services: llama-server (inference) + open-webui (web interface)

services:
  # ==========================================================================
  # llama-server: Inference engine for gpt-oss-120b
  # Includes: llama-bench, llama-cli, llama-quantize, and other utilities
  # ==========================================================================
  llama-server:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest
    container_name: strieber-llama-server

    # Restart policy
    restart: unless-stopped

    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]

    # Shared memory for CUDA operations
    # 16g sufficient for 20b, may need adjustment for very large models
    shm_size: 16g

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container
    # IMPORTANT: Make sure this is accessible from remote machines
    ports:
      - "${LLAMA_PORT:-8000}:8000"

    # Volume mounts
    volumes:
      # Models directory (write access enabled for llama-cli downloads)
      # Mount actual models directory, not symlink
      - /home/trevor/models:/models
      # Optional: cache directory for prompt caching
      # - ./cache:/cache

    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - LLAMA_ARG_HOST=${LLAMA_HOST:-0.0.0.0}
      - LLAMA_ARG_PORT=8000

    # Resource limits (optional, uncomment if needed)
    # ulimits:
    #   memlock: -1
    #   stack: 67108864

    # Server command optimized for gpt-oss-20b (testing) and gpt-oss-120b (production)
    # Override MODEL_FILE, CONTEXT_SIZE, GPU_LAYERS in .env for different models
    command:
      - "-m"
      - "/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "${CONTEXT_SIZE:-131072}"     # Context window: 128k for extended context
      - "--n-gpu-layers"
      - "${GPU_LAYERS:-999}"           # Auto-detect layers (works for both models)
      - "--jinja"                      # Enable chat template support
      - "-b"
      - "2048"                         # Batch size: up from default 512 for better throughput
      - "-ub"
      - "2048"                         # Ubatch size: physical batch size (same as -b for high-end hardware)
      - "--flash-attn"
      - "on"                           # Flash attention: 15% speedup, reduces VRAM usage
      - "--cont-batching"              # Continuous batching: better throughput with multiple requests
      - "--parallel"
      - "1"                            # Concurrent requests: 1 for full context per request
      - "--no-mmap"                    # Disable mmap for better performance with unified memory
      - "--cache-type-k"
      - "f16"                          # KV cache precision: balance between speed and VRAM
      - "--cache-type-v"
      - "f16"

    # Network configuration for MCP server communication
    networks:
      - strieber-net

  # ==========================================================================
  # open-webui: Web interface for llama-server
  # Connects to llama-server OpenAI-compatible API endpoint
  # ==========================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: strieber-open-webui

    # Restart policy
    restart: unless-stopped

    # Port mapping: host:container
    ports:
      - "${OPENWEBUI_PORT:-3000}:8080"

    # Volume for persistent data
    volumes:
      # Host bind mount for direct filesystem access and backups
      - ./open-webui-data:/app/backend/data

    # Environment variables
    environment:
      # Connect to llama-server using Docker container name
      - OPENAI_API_BASE_URL=http://strieber-llama-server:8000/v1
      - OPENAI_API_KEY=sk-open-webui-local  # Dummy key for local usage
      # Enable direct MCP server connections (native tool discovery)
      - ENABLE_DIRECT_CONNECTIONS=true

    # Depends on llama-server and MCP servers being started
    depends_on:
      - llama-server
      - mcp-weather
      - mcp-jina-reader
      - mcp-web-search
      - mcp-code-interpreter

    # Network configuration for inter-service communication
    networks:
      - strieber-net

  # ==========================================================================
  # code-executor: Sandboxed Python execution environment
  # Used by code-interpreter MCP server for safe code execution
  # ==========================================================================
  code-executor:
    build:
      context: ./backend/tools/code-executor
      dockerfile: Dockerfile
    image: code-executor:latest
    container_name: code-executor
    restart: unless-stopped
    networks:
      - strieber-net

  # ==========================================================================
  # MCP Tool Servers (Phase 2)
  # ==========================================================================

  # mcp-weather: Weather forecast service
  # Uses: Open-Meteo API (no API key required)
  # Provides: Current weather, daily forecast, weekly forecast
  mcp-weather:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: weather
    image: strieber-mcp-weather:latest
    container_name: strieber-mcp-weather
    restart: unless-stopped
    ports:
      - "8100:8000"
    environment:
      - PORT=8000
    networks:
      - strieber-net

  # mcp-page-reader: Local web page content extraction
  # Uses: Local ReaderLM-v2 + Playwright + Mozilla Readability
  # Provides: Structured page data (content, metadata, links, images)
  mcp-page-reader:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: page_reader
    image: strieber-mcp-page-reader:latest
    container_name: strieber-mcp-page-reader
    restart: unless-stopped
    ports:
      - "8101:8000"
    environment:
      - PORT=8000
      - READERLM_BASE_URL=http://llama-server-reader:8004
      - PLAYWRIGHT_BASE_URL=http://playwright-fetcher:8005
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      llama-server-reader:
        condition: service_healthy
      playwright-fetcher:
        condition: service_healthy
    networks:
      - strieber-net

  # mcp-web-search: Brave Search integration
  # Uses: Brave Search API (requires BRAVE_API_KEY)
  # Provides: Web search, news search with query expansion
  mcp-web-search:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: web_search
    image: strieber-mcp-web-search:latest
    container_name: strieber-mcp-web-search
    restart: unless-stopped
    ports:
      - "8102:8000"
    environment:
      - PORT=8000
      - BRAVE_API_KEY=${BRAVE_API_KEY:-}
      - LLAMA_BASE_URL=http://llama-server:8000
      - MODEL_NAME=${MODEL_FILE:-gpt-oss-20b}
    depends_on:
      llama-server:
        condition: service_healthy
    networks:
      - strieber-net

  # mcp-code-interpreter: Sandboxed Python execution
  # Uses: Docker container isolation for safe code execution
  # Provides: Python code execution with matplotlib figure capture
  mcp-code-interpreter:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: code_interpreter
    image: strieber-mcp-code-interpreter:latest
    container_name: strieber-mcp-code-interpreter
    restart: unless-stopped
    ports:
      - "8103:8000"
    environment:
      - PORT=8000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - code-executor
    networks:
      - strieber-net

  # ==========================================================================
  # playwright-fetcher: Headless browser for HTML fetching
  # Used by jina-reader MCP server for JavaScript-rendered pages
  # ==========================================================================
  playwright-fetcher:
    build:
      context: ./backend/tools/playwright-fetcher
      dockerfile: Dockerfile
    image: strieber-playwright-fetcher:latest
    container_name: strieber-playwright-fetcher
    restart: unless-stopped
    ports:
      - "8005:8005"
    cap_add:
      - SYS_ADMIN  # Required for Chromium sandbox
    shm_size: 2g  # Shared memory for browser
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - strieber-net

  # ==========================================================================
  # llama-server-reader: ReaderLM-v2 inference for HTML-to-Markdown conversion
  # Lightweight 1.5B model optimized for document processing
  # ==========================================================================
  llama-server-reader:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest  # Reuse same image as main llama-server
    container_name: strieber-llama-server-reader
    restart: unless-stopped

    # GPU configuration (uses less resources than main model)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Only needs 1 GPU for this small model
              capabilities: [gpu]

    shm_size: 4g  # Less memory needed for 1.5B model
    ipc: host

    ports:
      - "8004:8004"

    volumes:
      - /home/trevor/models:/models

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8004

    # ReaderLM-v2 optimized configuration
    command:
      - "-m"
      - "/models/ReaderLM-v2.Q4_K_M.gguf"
      - "--port"
      - "8004"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "65536"  # 64k context (ReaderLM supports up to 512k)
      - "--n-gpu-layers"
      - "999"  # Auto-detect all layers
      - "--jinja"
      - "--chat-template"
      - "chatml"  # ReaderLM uses ChatML format
      - "-b"
      - "512"  # Smaller batch size for this task
      - "--flash-attn"
      - "on"
      - "--parallel"
      - "2"  # Can handle more concurrent requests (small model)

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s  # Faster loading for small model

    networks:
      - strieber-net

networks:
  strieber-net:
    driver: bridge
