# ============================================================================
# strieber-gpt-3: LLama-Server + Open WebUI Docker Compose
# ============================================================================
# Docker compose file for running llama.cpp inference engine with Open WebUI
# on a GPU machine (e.g., DGX Spark)
#
# Usage:
#   docker compose up -d          # Start both services
#   docker compose up -d llama-server   # Start only llama-server
#
# Access:
#   - llama-server OpenAI API: http://localhost:8000
#   - Open WebUI: http://localhost:3000
#
# Services: llama-server (inference) + open-webui (web interface)

services:
  # ==========================================================================
  # llama-server: Inference engine for gpt-oss-120b
  # Includes: llama-bench, llama-cli, llama-quantize, and other utilities
  # ==========================================================================
  llama-server:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest
    container_name: strieber-llama-server

    # Restart policy
    restart: unless-stopped

    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]

    # Shared memory for CUDA operations
    # 16g sufficient for 20b, may need adjustment for very large models
    shm_size: 16g

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container
    # IMPORTANT: Make sure this is accessible from remote machines
    ports:
      - "${LLAMA_PORT:-8000}:8000"

    # Volume mounts
    volumes:
      # Models directory (write access enabled for llama-cli downloads)
      # Mount actual models directory, not symlink
      - /home/trevor/models:/models
      # Optional: cache directory for prompt caching
      # - ./cache:/cache

    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - LLAMA_ARG_HOST=${LLAMA_HOST:-0.0.0.0}
      - LLAMA_ARG_PORT=8000
      # Logging configuration (see .env for control variables)
      - LLAMA_LOG_VERBOSITY=${LLAMA_LOG_VERBOSITY:-0}
      - LLAMA_LOG_TIMESTAMPS=${LLAMA_LOG_TIMESTAMPS:-true}
      - LLAMA_LOG_PREFIX=${LLAMA_LOG_PREFIX:-true}

    # Resource limits (optional, uncomment if needed)
    # ulimits:
    #   memlock: -1
    #   stack: 67108864

    # Server command optimized for gpt-oss-20b (testing) and gpt-oss-120b (production)
    # Override MODEL_FILE, CONTEXT_SIZE, GPU_LAYERS in .env for different models
    command:
      - "-m"
      - "/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "${CONTEXT_SIZE:-131072}"     # Context window: 128k for extended context
      - "--n-gpu-layers"
      - "${GPU_LAYERS:-999}"           # Auto-detect layers (works for both models)
      - "--jinja"                      # Enable chat template support
      - "-b"
      - "2048"                         # Batch size: up from default 512 for better throughput
      - "-ub"
      - "2048"                         # Ubatch size: physical batch size (same as -b for high-end hardware)
      - "--flash-attn"
      - "on"                           # Flash attention: 15% speedup, reduces VRAM usage
      - "--cont-batching"              # Continuous batching: better throughput with multiple requests
      - "--parallel"
      - "1"                            # Concurrent requests: 1 for full context per request
      - "--no-mmap"                    # Disable mmap for better performance with unified memory
      - "--cache-type-k"
      - "f16"                          # KV cache precision: balance between speed and VRAM
      - "--cache-type-v"
      - "f16"

    # Network configuration for MCP server communication
    networks:
      - strieber-net

  # ==========================================================================
  # llama-server-readerlm: Inference engine for ReaderLM-v2 model
  # ABOUTME: Local HTML-to-Markdown conversion without external API calls
  # Dedicated instance for privacy: URLs and content never leave this server
  # ==========================================================================
  llama-server-readerlm:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest  # Reuse same image
    container_name: strieber-llama-server-readerlm

    # Restart policy
    restart: unless-stopped

    # GPU configuration (shares GPU 0 with main llama-server)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Request 1 GPU, will share with llama-server
              capabilities: [gpu]

    # Shared memory for CUDA operations
    shm_size: 8g  # Less than main server (smaller model)

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container (8001 for ReaderLM vs 8000 for main)
    ports:
      - "8001:8000"

    # Volume mounts
    volumes:
      # Models directory
      - /home/trevor/models:/models

    # Environment variables
    environment:
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_LOG_VERBOSITY=0
      - LLAMA_LOG_TIMESTAMPS=true
      - LLAMA_LOG_PREFIX=true

    # Server command optimized for ReaderLM-v2 (1.5B params)
    # Smaller model = lower latency, lower VRAM requirements
    # Extended context: Model trained to 256K, can extrapolate to 512K tokens
    command:
      - "-m"
      - "/models/ReaderLM-v2-Q4_K_M.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "131072"                          # Context: 128k tokens (max safe, matches main server)
      - "--n-gpu-layers"
      - "999"                             # Auto-detect layers for GPU offload
      - "--jinja"                         # Enable chat template support
      - "-b"
      - "1024"                            # Batch size: smaller for smaller model
      - "-ub"
      - "1024"                            # Ubatch size: physical batch size
      - "--flash-attn"
      - "on"                              # Flash attention: efficiency
      - "--cont-batching"                 # Continuous batching
      - "--parallel"
      - "2"                               # Allow up to 2 concurrent requests
      - "-t"
      - "0.1"                             # Temperature: near-zero for deterministic conversion
      - "--no-mmap"                       # Disable mmap

    # Network configuration for MCP server communication
    networks:
      - strieber-net

  # ==========================================================================
  # open-webui: Web interface for llama-server
  # Connects to llama-server OpenAI-compatible API endpoint
  # ==========================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.6.36
    container_name: strieber-open-webui

    # Restart policy
    restart: unless-stopped

    # Port mapping: host:container
    ports:
      - "${OPENWEBUI_PORT:-3000}:8080"

    # Volume for persistent data
    volumes:
      # Host bind mount for direct filesystem access and backups
      - ./open-webui-data:/app/backend/data

    # Environment variables
    environment:
      # Connect to llama-server using Docker container name
      - OPENAI_API_BASE_URL=http://strieber-llama-server:8000/v1
      - OPENAI_API_KEY=sk-open-webui-local  # Dummy key for local usage
      # Enable direct MCP server connections (native tool discovery)
      - ENABLE_DIRECT_CONNECTIONS=true

    # Depends on llama-server and MCP servers being started
    depends_on:
      - llama-server
      - mcp-weather
      - mcp-web-search
      - mcp-code-interpreter
      - mcp-reader

    # Network configuration for inter-service communication
    networks:
      - strieber-net

  # ==========================================================================
  # code-executor: Sandboxed Python execution environment
  # Used by code-interpreter MCP server for safe code execution
  # ==========================================================================
  code-executor:
    build:
      context: ./backend/tools/code-executor
      dockerfile: Dockerfile
    image: code-executor:latest
    container_name: code-executor
    restart: unless-stopped
    networks:
      - strieber-net

  # ==========================================================================
  # MCP Tool Servers (Phase 2)
  # ==========================================================================

  # mcp-weather: Weather forecast service
  # Uses: Open-Meteo API (no API key required)
  # Provides: Current weather, daily forecast, weekly forecast
  mcp-weather:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: weather
    image: strieber-mcp-weather:latest
    container_name: strieber-mcp-weather
    restart: unless-stopped
    ports:
      - "8100:8000"
    environment:
      - PORT=8000
    networks:
      - strieber-net

  # mcp-web-search: Brave Search integration
  # Uses: Brave Search API (requires BRAVE_API_KEY)
  # Provides: Web search, news search with query expansion
  mcp-web-search:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: web_search
    image: strieber-mcp-web-search:latest
    container_name: strieber-mcp-web-search
    restart: unless-stopped
    ports:
      - "8102:8000"
    environment:
      - PORT=8000
      - BRAVE_API_KEY=${BRAVE_API_KEY:-}
      - LLAMA_BASE_URL=http://llama-server:8000
      - MODEL_NAME=${MODEL_FILE:-gpt-oss-20b}
    depends_on:
      llama-server:
        condition: service_healthy
    networks:
      - strieber-net

  # mcp-code-interpreter: Sandboxed Python execution
  # Uses: Docker container isolation for safe code execution
  # Provides: Python code execution with matplotlib figure capture
  mcp-code-interpreter:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: code_interpreter
    image: strieber-mcp-code-interpreter:latest
    container_name: strieber-mcp-code-interpreter
    restart: unless-stopped
    ports:
      - "8103:8000"
    environment:
      - PORT=8000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - code-executor
    networks:
      - strieber-net

  # ==========================================================================
  # playwright-scraper: Web scraping service with JavaScript rendering
  # ABOUTME: Provides REST API for fetching URLs with Playwright
  # Used by local_reader MCP server for privacy-preserving web scraping
  # ==========================================================================
  playwright-scraper:
    build:
      context: ./backend/tools/playwright-scraper
      dockerfile: Dockerfile
    image: strieber-playwright-scraper:latest
    container_name: strieber-playwright-scraper
    restart: unless-stopped
    environment:
      - HEADLESS=true
    # No ports exposed - internal only
    networks:
      - strieber-net
    ipc: host  # Critical for Chromium stability
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health', timeout=5.0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==========================================================================
  # mcp-reader: Privacy-first web content extraction
  # ABOUTME: Local HTML-to-Markdown conversion with instruction-based extraction
  # Uses: Playwright scraper + ReaderLM-v2 for complete private pipeline
  # ==========================================================================
  mcp-reader:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: reader
    image: strieber-mcp-reader:latest
    container_name: strieber-mcp-reader
    restart: unless-stopped
    ports:
      - "8104:8000"
    environment:
      - PORT=8000
      - SCRAPER_ENDPOINT=http://playwright-scraper:8000
      - LLAMA_ENDPOINT=http://llama-server-readerlm:8000
    depends_on:
      playwright-scraper:
        condition: service_healthy
      llama-server-readerlm:
        condition: service_healthy
    networks:
      - strieber-net

networks:
  strieber-net:
    driver: bridge
