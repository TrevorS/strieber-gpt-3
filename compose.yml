# ============================================================================
# strieber-gpt-3: LLama-Server + Open WebUI Docker Compose
# ============================================================================
# Docker compose file for running llama.cpp inference engine with Open WebUI
# on a GPU machine (e.g., DGX Spark)
#
# Usage:
#   docker compose up -d          # Start both services
#   docker compose up -d llama-server   # Start only llama-server
#
# Access:
#   - llama-server OpenAI API: http://localhost:8000
#   - Open WebUI: http://localhost:3000
#
# Services: llama-server (inference) + open-webui (web interface)

services:
  # ==========================================================================
  # llama-server: Inference engine for gpt-oss-120b
  # Includes: llama-bench, llama-cli, llama-quantize, and other utilities
  # ==========================================================================
  llama-server:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest
    container_name: strieber-llama-server

    # Restart policy
    restart: unless-stopped

    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]

    # Shared memory for CUDA operations
    # 16g sufficient for 20b, may need adjustment for very large models
    shm_size: 16g

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container (9010 = LLM range, main orchestrator)
    # IMPORTANT: Make sure this is accessible from remote machines
    ports:
      - "${LLAMA_PORT:-9010}:8000"

    # Volume mounts
    volumes:
      # Models directory: GPT-OSS models for main inference engine
      - /home/trevor/models/llama-cpp/gpt-oss:/models
      # Optional: cache directory for prompt caching
      # - ./cache:/cache

    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - LLAMA_ARG_HOST=${LLAMA_HOST:-0.0.0.0}
      - LLAMA_ARG_PORT=8000
      # Logging configuration (see .env for control variables)
      - LLAMA_LOG_VERBOSITY=${LLAMA_LOG_VERBOSITY:-0}
      - LLAMA_LOG_TIMESTAMPS=${LLAMA_LOG_TIMESTAMPS:-true}
      - LLAMA_LOG_PREFIX=${LLAMA_LOG_PREFIX:-true}

    # Resource limits (optional, uncomment if needed)
    # ulimits:
    #   memlock: -1
    #   stack: 67108864

    # Server command optimized for gpt-oss-20b (testing) and gpt-oss-120b (production)
    # Override MODEL_FILE, CONTEXT_SIZE, GPU_LAYERS in .env for different models
    command:
      - "-m"
      - "/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "${CONTEXT_SIZE:-131072}"     # Context window: 128k for extended context
      - "--n-gpu-layers"
      - "${GPU_LAYERS:-999}"           # Auto-detect layers (works for both models)
      - "--jinja"                      # Enable chat template support
      - "-b"
      - "512"                          # Batch size: reduced from 2048 to prevent KV cache exhaustion
      - "-ub"
      - "512"                          # Ubatch size: physical batch size (reduced for memory stability)
      - "--flash-attn"
      - "on"                           # Flash attention: 15% speedup, reduces VRAM usage
      - "--cont-batching"              # Continuous batching: better throughput with multiple requests
      - "--parallel"
      - "1"                            # Concurrent requests: 1 for full context per request
      - "--no-mmap"                    # Disable mmap for better performance with unified memory
      - "--cache-type-k"
      - "f16"                          # KV cache precision: balance between speed and VRAM
      - "--cache-type-v"
      - "f16"

    # Network configuration for MCP server communication
    networks:
      - strieber-net

  # ==========================================================================
  # llama-server-readerlm: Inference engine for ReaderLM-v2 model
  # ABOUTME: Local HTML-to-Markdown conversion without external API calls
  # Dedicated instance for privacy: URLs and content never leave this server
  # ==========================================================================
  llama-server-readerlm:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest  # Reuse same image
    container_name: strieber-llama-server-readerlm

    # Restart policy
    restart: unless-stopped

    # GPU configuration (shares GPU 0 with main llama-server)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Request 1 GPU, will share with llama-server
              capabilities: [gpu]

    # Shared memory for CUDA operations
    shm_size: 8g  # Less than main server (smaller model)

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container (9030 = LLM range, ReaderLM)
    ports:
      - "9030:8000"

    # Volume mounts
    volumes:
      # Models directory: ReaderLM for HTML-to-Markdown conversion
      - /home/trevor/models/llama-cpp/specialized:/models

    # Environment variables
    environment:
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_LOG_VERBOSITY=0
      - LLAMA_LOG_TIMESTAMPS=true
      - LLAMA_LOG_PREFIX=true

    # Server command optimized for ReaderLM-v2 (1.5B params)
    # Smaller model = lower latency, lower VRAM requirements
    # Extended context: Model trained to 256K, can extrapolate to 512K tokens
    command:
      - "-m"
      - "/models/ReaderLM-v2-Q4_K_M.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "131072"                          # Context: 128k tokens (max safe, matches main server)
      - "--n-gpu-layers"
      - "999"                             # Auto-detect layers for GPU offload
      - "--jinja"                         # Enable chat template support
      - "-b"
      - "1024"                            # Batch size: smaller for smaller model
      - "-ub"
      - "1024"                            # Ubatch size: physical batch size
      - "--flash-attn"
      - "on"                              # Flash attention: efficiency
      - "--cont-batching"                 # Continuous batching
      - "--parallel"
      - "1"                               # Single concurrent request (full 131K context per request)
      - "-t"
      - "0.1"                             # Temperature: near-zero for deterministic conversion
      - "--no-mmap"                       # Disable mmap

    # Network configuration for MCP server communication
    networks:
      - strieber-net

  # ==========================================================================
  # llama-server-qwen-vl: Vision-language model for task routing and image analysis
  # ABOUTME: Qwen3-VL-2B multimodal model for vision tasks and text generation
  # Supports: Image understanding, visual question answering, task classification
  # ==========================================================================
  llama-server-qwen-vl:
    build:
      context: .
      dockerfile: Dockerfile.llamacpp
      args:
        CUDA_VERSION: "13.0.1"
        UBUNTU_VERSION: "22.04"
        CUDA_DOCKER_ARCH: "121"  # Blackwell GB10

    image: strieber-llama-server:latest  # Reuse same image
    container_name: strieber-llama-server-qwen-vl

    # Restart policy
    restart: unless-stopped

    # GPU configuration (shares GPU 0 with other llama-server instances)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Request 1 GPU, will share with other instances
              capabilities: [gpu]

    # Shared memory for CUDA operations
    shm_size: 4g  # Small model, minimal overhead

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container (9020 = LLM range, Qwen-VL task + vision)
    ports:
      - "9020:8000"

    # Volume mounts
    volumes:
      # Models directory: Qwen-VL vision-language model and mmproj
      - /home/trevor/models/llama-cpp/qwen-vl:/models

    # Environment variables
    environment:
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_LOG_VERBOSITY=0
      - LLAMA_LOG_TIMESTAMPS=true
      - LLAMA_LOG_PREFIX=true

    # Server command optimized for Qwen3-VL-2B (multimodal model)
    # Supports both text generation and vision tasks (image analysis)
    command:
      - "-m"
      - "/models/Qwen3-VL-2B-Instruct-Q4_K_M.gguf"
      - "--mmproj"
      - "/models/mmproj-F16.gguf"             # Multimodal projector for vision
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "32768"                               # Context: 32k tokens (standard for 2B models)
      - "--n-gpu-layers"
      - "999"                                 # Auto-detect layers for GPU offload
      - "--jinja"                             # Enable chat template support
      - "-b"
      - "512"                                 # Batch size: standard for 2B model
      - "-ub"
      - "512"                                 # Ubatch size: physical batch size
      - "--flash-attn"
      - "on"                                  # Flash attention: efficiency
      - "--cont-batching"                     # Continuous batching
      - "--parallel"
      - "1"                                   # Single concurrent request
      - "--no-mmap"                           # Disable mmap

    # Network configuration for MCP server communication
    networks:
      - strieber-net

  # ==========================================================================
  # embeddinggemma: EmbeddingGemma-300M embedding model server
  # ABOUTME: Lightweight embedding inference for RAG and semantic search
  # Provides OpenAI-compatible /v1/embeddings endpoint for document vectorization
  # ==========================================================================
  embeddinggemma:
    image: strieber-llama-server:latest  # Reuse same custom-built image
    container_name: strieber-embeddinggemma
    restart: unless-stopped

    # GPU configuration (shares GPU 0 with other services)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Request 1 GPU, will share with other instances
              capabilities: [gpu]

    # Shared memory for CUDA operations
    shm_size: 2g  # Minimal for small embedding model (300M params)

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container (9050 = embedding range)
    ports:
      - "9050:8000"

    # Volume mounts
    volumes:
      # Models directory: EmbeddingGemma embedding model
      - /home/trevor/models/llama-cpp/specialized:/models

    # Environment variables
    environment:
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_LOG_VERBOSITY=0
      - LLAMA_LOG_TIMESTAMPS=true
      - LLAMA_LOG_PREFIX=true

    # Server command optimized for EmbeddingGemma-300M
    # Embedding mode with mean pooling for OpenAI API compatibility
    command:
      - "-m"
      - "/models/embeddinggemma-300M-qat-Q4_0.gguf"
      - "--embedding"                            # Enable embedding mode
      - "--pooling"
      - "mean"                                   # Pooling strategy: mean (required for /v1/embeddings)
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "-c"
      - "8192"                                   # Context size: 8k tokens (EmbeddingGemma max)
      - "--n-gpu-layers"
      - "999"                                    # Auto-detect layers for GPU offload
      - "-b"
      - "128"                                    # Batch size: embeddings can handle larger batches
      - "-ub"
      - "128"                                    # Ubatch size: physical batch size
      - "--no-mmap"                              # Disable mmap

    # Network configuration for inter-service communication
    networks:
      - strieber-net

  # ==========================================================================
  # comfyui: Advanced image generation workflows and Stable Diffusion UI
  # ABOUTME: ComfyUI node-based interface for image generation and manipulation
  # Shares GPU 0 with other services, uses shared model storage
  # ==========================================================================
  comfyui:
    build:
      context: ./comfyui
      dockerfile: Dockerfile
      args:
        UID: 1000
        GID: 1000
    image: strieber-comfyui:latest
    container_name: strieber-comfyui
    restart: unless-stopped

    # GPU configuration (shares GPU 0 with other services)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Shared memory for CUDA operations
    shm_size: 4g

    # IPC mode for better GPU communication
    ipc: host

    # Port mapping: host:container (9040 = ComfyUI)
    ports:
      - "${COMFYUI_PORT:-9040}:8188"

    # Volume mounts
    volumes:
      # Models: shared storage across projects
      - /home/trevor/models/comfyui:/opt/ComfyUI/models
      # Data: inputs, outputs, custom nodes (project-local)
      - ./comfyui-data/input:/opt/ComfyUI/input
      - ./comfyui-data/output:/opt/ComfyUI/output
      - ./comfyui-data/custom_nodes:/opt/ComfyUI/custom_nodes
      # Caches: HuggingFace and PyTorch (project-local to avoid conflicts)
      - ./comfyui-data/hf-cache:/home/app/.cache/huggingface
      - ./comfyui-data/torch-cache:/home/app/.cache/torch

    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - COMFY_LISTEN_HOST=0.0.0.0
      - COMFY_PORT=8188
      - DOWNLOAD_SD15=${COMFYUI_DOWNLOAD_SD15:-0}

    # Command-line arguments
    command:
      - "--enable-cors-header"
      - "*"
      - "--disable-auto-launch"

    # Resource limits
    ulimits:
      memlock: -1
      stack: 67108864

    # Healthcheck
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:8188"]
      interval: 30s
      timeout: 5s
      start_period: 20s
      retries: 10

    # Network configuration
    networks:
      - strieber-net

  # ==========================================================================
  # open-webui: Web interface for llama-server
  # ==========================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.6.36
    container_name: strieber-open-webui

    # Restart policy
    restart: unless-stopped

    # Port mapping: host:container (9200 = Web UI range)
    ports:
      - "${OPENWEBUI_PORT:-9200}:8080"

    # Volume for persistent data
    volumes:
      # Host bind mount for direct filesystem access and backups
      - ./open-webui-data:/app/backend/data

    # Environment variables
    environment:
      # Connect directly to llama-server for gpt-oss-120b
      - OPENAI_API_BASE_URL=http://strieber-llama-server:8000/v1
      - OPENAI_API_KEY=none
      # Enable direct MCP server connections (native tool discovery)
      - ENABLE_DIRECT_CONNECTIONS=true

    # Depends on llama-server and MCP servers being started
    depends_on:
      - llama-server
      - mcp-weather
      - mcp-web-search
      - mcp-code-interpreter
      - mcp-reader

    # Network configuration for inter-service communication
    networks:
      - strieber-net

  # ==========================================================================
  # code-executor: Sandboxed Python execution environment
  # Used by code-interpreter MCP server for safe code execution
  # Note: This is ephemeral and invoked via docker run, not docker compose up
  # ==========================================================================
  code-executor:
    build:
      context: ./backend/tools/code-executor
      dockerfile: Dockerfile
    image: code-executor:latest
    container_name: code-executor
    restart: "no"
    networks:
      - strieber-net
    profiles:
      - tools

  # ==========================================================================
  # MCP Tool Servers (Phase 2)
  # ==========================================================================

  # mcp-weather: Weather forecast service
  # Uses: Open-Meteo API (no API key required)
  # Provides: Current weather, daily forecast, weekly forecast
  mcp-weather:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: weather
    image: strieber-mcp-weather:latest
    container_name: strieber-mcp-weather
    restart: unless-stopped
    ports:
      - "9100:8000"
    environment:
      - PORT=8000
    networks:
      - strieber-net

  # mcp-web-search: Brave Search integration
  # Uses: Brave Search API (requires BRAVE_API_KEY)
  # Provides: Web search, news search with query expansion
  mcp-web-search:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: web_search
    image: strieber-mcp-web-search:latest
    container_name: strieber-mcp-web-search
    restart: unless-stopped
    ports:
      - "9110:8000"
    environment:
      - PORT=8000
      - BRAVE_API_KEY=${BRAVE_API_KEY:-}
      - LLAMA_BASE_URL=http://llama-server:8000
      - MODEL_NAME=${MODEL_FILE:-gpt-oss-20b}
    depends_on:
      llama-server:
        condition: service_healthy
    networks:
      - strieber-net

  # mcp-code-interpreter: Sandboxed Python execution
  # Uses: Docker container isolation for safe code execution
  # Provides: Python code execution with matplotlib figure capture
  # Note: code-executor image built separately, invoked via docker run
  mcp-code-interpreter:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: code_interpreter
    image: strieber-mcp-code-interpreter:latest
    container_name: strieber-mcp-code-interpreter
    restart: unless-stopped
    ports:
      - "9120:8000"
    environment:
      - PORT=8000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - strieber-net

  # ==========================================================================
  # playwright-scraper: Web scraping service with JavaScript rendering
  # ABOUTME: Provides REST API for fetching URLs with Playwright
  # Used by local_reader MCP server for privacy-preserving web scraping
  # ==========================================================================
  playwright-scraper:
    build:
      context: ./backend/tools/playwright-scraper
      dockerfile: Dockerfile
    image: strieber-playwright-scraper:latest
    container_name: strieber-playwright-scraper
    restart: unless-stopped
    environment:
      - HEADLESS=true
    # No ports exposed - internal only
    networks:
      - strieber-net
    ipc: host  # Critical for Chromium stability
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health', timeout=5.0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==========================================================================
  # mcp-reader: Privacy-first web content extraction
  # ABOUTME: Local HTML-to-Markdown conversion with instruction-based extraction
  # Uses: Playwright scraper + ReaderLM-v2 for complete private pipeline
  # ==========================================================================
  mcp-reader:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: reader
    image: strieber-mcp-reader:latest
    container_name: strieber-mcp-reader
    restart: unless-stopped
    ports:
      - "9130:8000"
    environment:
      - PORT=8000
      - SCRAPER_ENDPOINT=http://playwright-scraper:8000
      - LLAMA_ENDPOINT=http://llama-server-readerlm:8000
    depends_on:
      playwright-scraper:
        condition: service_healthy
      llama-server-readerlm:
        condition: service_healthy
    networks:
      - strieber-net

  # mcp-comfy-qwen: Text-to-image and image editing with Qwen models
  # ABOUTME: MCP server for ComfyUI-based Qwen image generation with Lightning LoRA support
  # Uses: Split model architecture (UNETLoader, CLIPLoader, VAELoader) with quality presets
  # ==========================================================================
  mcp-comfy-qwen:
    build:
      context: ./backend/tools/mcp_servers
      dockerfile: Dockerfile.mcp-server
      args:
        SERVER_MODULE: comfy_qwen
    image: strieber-mcp-comfy-qwen:latest
    container_name: strieber-mcp-comfy-qwen
    restart: unless-stopped
    ports:
      - "9140:8000"
    environment:
      - PORT=8000
      - COMFY_URL=http://comfyui:8188
      - OWUI_BASE_URL=http://open-webui:8080
      - OWUI_PUBLIC_URL=http://spark-ebf0.tailbb09a1.ts.net:9200
      - OWUI_API_TOKEN=sk-5467fee6567144d3a8b1758ebb5bb843
    depends_on:
      comfyui:
        condition: service_healthy
    networks:
      - strieber-net

networks:
  strieber-net:
    driver: bridge
