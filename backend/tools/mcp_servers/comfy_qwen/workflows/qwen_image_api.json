{
  "_comment": "Qwen Image (txt2img) workflow with Lightning LoRA support and split model loaders.",
  "_instructions": "Uses UNETLoader, CLIPLoader, and VAELoader for split model components. LoRA applied conditionally based on quality preset.",
  "_node_mapping": {
    "unet_loader": "Node ID 1 - UNETLoader for diffusion model",
    "clip_loader": "Node ID 8 - CLIPLoader for text encoder",
    "vae_loader": "Node ID 9 - VAELoader for VAE decoder",
    "lora_loader": "Node ID 10 - LoraLoaderModelOnly for Lightning LoRA (bypassed for standard/high)",
    "positive_prompt": "Node ID 2 - CLIPTextEncode for positive prompt",
    "negative_prompt": "Node ID 3 - CLIPTextEncode for negative prompt",
    "empty_latent": "Node ID 4 - EmptyLatentImage (width, height, batch_size)",
    "sampler": "Node ID 5 - KSampler (seed, steps, cfg)",
    "vae_decode": "Node ID 6 - VAEDecode",
    "save_image": "Node ID 7 - SaveImage"
  },
  "1": {
    "inputs": {
      "unet_name": "qwen_image_fp8_e4m3fn.safetensors",
      "weight_dtype": "fp8_e4m3fn"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "Load Diffusion Model"
    }
  },
  "8": {
    "inputs": {
      "clip_name": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
      "type": "qwen_image"
    },
    "class_type": "CLIPLoader",
    "_meta": {
      "title": "Load CLIP Text Encoder"
    }
  },
  "9": {
    "inputs": {
      "vae_name": "qwen_image_vae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "10": {
    "inputs": {
      "model": [
        "1",
        0
      ],
      "lora_name": "Qwen-Image-Lightning-8steps-V2.0-bf16.safetensors",
      "strength_model": 1.0
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "Load Lightning LoRA (Optional)"
    },
    "_note": "This node can be bypassed by setting mode=4. The server.py will set mode=4 when quality='standard' or 'high'."
  },
  "2": {
    "inputs": {
      "text": "PLACEHOLDER_POSITIVE_PROMPT",
      "clip": [
        "8",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive)"
    }
  },
  "3": {
    "inputs": {
      "text": "PLACEHOLDER_NEGATIVE_PROMPT",
      "clip": [
        "8",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Negative)"
    }
  },
  "4": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "5": {
    "inputs": {
      "seed": 0,
      "steps": 20,
      "cfg": 5.0,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 1.0,
      "model": [
        "10",
        0
      ],
      "positive": [
        "2",
        0
      ],
      "negative": [
        "3",
        0
      ],
      "latent_image": [
        "4",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    },
    "_note": "Model input comes from LoRA loader (node 10). When LoRA is bypassed, this will still work as the LoRA passthrough outputs the model."
  },
  "6": {
    "inputs": {
      "samples": [
        "5",
        0
      ],
      "vae": [
        "9",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "7": {
    "inputs": {
      "filename_prefix": "qwen_image",
      "images": [
        "6",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  }
}
